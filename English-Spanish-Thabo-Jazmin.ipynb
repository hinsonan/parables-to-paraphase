{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7418c18b",
      "metadata": {},
      "source": [
        "# English-To-Spanish Translation\n",
        "\n",
        "This notebook demonstrates how to perform English-to-Spanish translation using Hugging Face transformer models. It is written for both beginners and advanced users and shows multiple approaches, device handling (CPU/GPU), batching, and how to translate different text types (single sentences, paragraphs, conversations).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67d29e9d",
      "metadata": {},
      "source": [
        "## Install requirements\n",
        "\n",
        "Run this cell in a fresh environment.\n",
        "\n",
        "```bash\n",
        "!pip install --upgrade pip\n",
        "!pip install transformers[sentencepiece] accelerate torch\n",
        "!pip install safetensors \n",
        "!pip install datasets tqdm huggingface-hub \n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "31b61a50",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# WE WANT TO DETECT THE DEVICE USED \n",
        "\n",
        "from typing import List, Iterable\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def detect_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    if torch.backends.mps.is_built():\n",
        "        return torch.device('mps')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "DEVICE = detect_device()\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e746fdcf",
      "metadata": {},
      "source": [
        "## Quick approach: `pipeline` (recommended for beginners)\n",
        "\n",
        "The `pipeline` API wraps tokenization, model loading and generation into a single, easy-to-use object. It's great for quick experiments and small-to-medium workloads. As we can see we have short sentences that don't even have long words, so the approach we're taking might be easy to understand. We are going to explore more involved approaches later on this natebook.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b4502911",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola, ¿cómo estás?\n",
            "La palabra de Dios está transformando el mundo.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"  \n",
        "translator = pipeline(\"translation\", model=model_name, device=0 if str(DEVICE).startswith('cuda') else -1)\n",
        "\n",
        "examples = [\n",
        "    \"Hello, how are you doing?\",\n",
        "    \"God's word is transforming the world.\"\n",
        "]\n",
        "\n",
        "for out in translator(examples, max_length=256):\n",
        "    print(out['translation_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2f5469",
      "metadata": {},
      "source": [
        "## Manual model loading (advanced users)\n",
        "\n",
        "Manually loading the tokenizer and model gives you full control over generation parameters (beam search, sampling, length penalties, etc.), and allows batching strategies that may be more efficient for large workloads. Why would we want to take this approach? Sometimes we want to customize our models and set parameters that suit a situation we want, in our case, translating English text(sermons) to Spanish.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9455d880",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['En el principio era el Verbo, y el Verbo estaba con Dios, y el Verbo era Dios.']\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer and model manually\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "def translate_batch_manual(texts: List[str], batch_device: torch.device = DEVICE, max_length: int = 256, num_beams: int = 4) -> List[str]:\n",
        "    if len(texts) == 0:\n",
        "        return []\n",
        "\n",
        "    encoded = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
        "    encoded = {k: v.to(batch_device) for k, v in encoded.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**encoded, max_length=max_length, num_beams=num_beams)\n",
        "\n",
        "    decoded = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]\n",
        "    return decoded\n",
        "\n",
        "print(translate_batch_manual([\" In the beginning was the Word, and the Word was with God, and the Word was God.\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48d3c78e",
      "metadata": {},
      "source": [
        "## Batch translation for large datasets\n",
        "\n",
        "When translating many sentences, divide them into batches (chunks) that fit in memory. We are likely going to be workking on tons of sentences, so it would be best to chop them up into pieces because the models have a maximum sequence length that they can't exceed or else we would lose some content, whenever the data gets truncated. \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b56dccdc",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating chunks: 100%|██████████| 4/4 [00:14<00:00,  3.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Esta es la frase número 1.', 'Esta es la frase número 2.', 'Esta es la frase número 3.', 'Esta es la frase número 4.', 'Esta es la frase número 5.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def chunked_iterable(iterable: Iterable, chunk_size: int):\n",
        "    it = iter(iterable)\n",
        "    while True:\n",
        "        chunk = []\n",
        "        try:\n",
        "            for _ in range(chunk_size):\n",
        "                chunk.append(next(it))\n",
        "        except StopIteration:\n",
        "            if chunk:\n",
        "                yield chunk\n",
        "            break\n",
        "        if chunk:\n",
        "            yield chunk\n",
        "\n",
        "def batch_translate(texts: List[str], chunk_size: int = 16, **generate_kwargs) -> List[str]:\n",
        "    translations = []\n",
        "    for chunk in tqdm(list(chunked_iterable(texts, chunk_size)), desc=\"Translating chunks\"):\n",
        "        translations.extend(translate_batch_manual(chunk, **generate_kwargs))\n",
        "    return translations\n",
        "\n",
        "sample_texts = [f\"This is sentence number {i}.\" for i in range(1, 31)]\n",
        "translated_sample = batch_translate(sample_texts, chunk_size=8)\n",
        "print(translated_sample[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6df5c43",
      "metadata": {},
      "source": [
        "## Handling paragraphs and longer inputs\n",
        "\n",
        "Transformer tokenizers have maximum sequence lengths. For very long paragraphs, consider splitting into sentences.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "56b39a32",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating chunks: 100%|██████████| 1/1 [00:07<00:00,  7.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dios usa nuestras pruebas para afinarnos para las bendiciones futuras que Él va a proveer para nosotros. En el presente no entendemos esto, pero con el tiempo todo tiene sentido.Él siempre está ahí para nosotros, y siempre estará\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def naive_sentence_split(paragraph: str) -> List[str]:\n",
        "    pieces = re.split(r'(?<=[.!?])\\s+', paragraph.strip())\n",
        "    pieces = [p.strip() for p in pieces if p.strip()]\n",
        "    return pieces\n",
        "\n",
        "def translate_paragraph(paragraph: str, chunk_size: int = 8) -> str:\n",
        "    sentences = naive_sentence_split(paragraph)\n",
        "    translations = batch_translate(sentences, chunk_size=chunk_size)\n",
        "    return ' '.join(translations)\n",
        "\n",
        "paragraph = (\n",
        "    \"God uses our trials to sharpen us for the future blessings He is going to provide for us. In the present we do not understand this, but with time it all makes sense.\" \\\n",
        "    \"He is always there for us, and will always be\"\n",
        "  \n",
        ")\n",
        "print(translate_paragraph(paragraph))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb574551",
      "metadata": {},
      "source": [
        "## Conversations / multi-turn contexts\n",
        "\n",
        "Translate each turn independently or concatenate context.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9bfb7e85",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating chunks: 100%|██████████| 1/1 [00:03<00:00,  3.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'speaker': 'Alice', 'text': 'Hey, are you coming to the bible study?', 'translation': 'Oye, ¿vienes al estudio de la Biblia?'}, {'speaker': 'Bob', 'text': \"Yes! I'll be there at 7 pm.\", 'translation': 'Bob: ¡Sí! Estaré allí a las 7 pm.'}, {'speaker': 'Alice', 'text': 'Great — see you then.', 'translation': 'Alice: Genial, nos vemos entonces.'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def translate_conversation(turns: List[dict], chunk_size: int = 8) -> List[dict]:\n",
        "    texts = [f\"{t['speaker']}: {t['text']}\" for t in turns]\n",
        "    translations = batch_translate(texts, chunk_size=chunk_size)\n",
        "    out = []\n",
        "    for t, tr in zip(turns, translations):\n",
        "        new = t.copy()\n",
        "        new['translation'] = tr\n",
        "        out.append(new)\n",
        "    return out\n",
        "\n",
        "conv = [\n",
        "    {\"speaker\": \"Alice\", \"text\": \"Hey, are you coming to the bible study?\"},\n",
        "    {\"speaker\": \"Bob\", \"text\": \"Yes! I'll be there at 7 pm.\"},\n",
        "    {\"speaker\": \"Alice\", \"text\": \"Great — see you then.\"}\n",
        "]\n",
        "print(translate_conversation(conv))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ff1cbd",
      "metadata": {},
      "source": [
        "## Saving translations\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "23e647e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 30 translations to es.txt and eng.tsv\n"
          ]
        }
      ],
      "source": [
        "def save_translations(inputs, translations, out_txt: str = \"es_translations.txt\", out_tsv: str = \"es_translations.tsv\"):\n",
        "    with open(out_txt, 'w', encoding='utf-8') as f_txt, open(out_tsv, 'w', encoding='utf-8') as f_tsv:\n",
        "        for inp, tr in zip(inputs, translations):\n",
        "            f_txt.write(tr + \"\\n\")\n",
        "            f_tsv.write(inp.replace('\\t',' ') + \"\\t\" + tr.replace('\\t',' ') + \"\\n\")\n",
        "    print(f\"Saved {len(translations)} translations to {out_txt} and {out_tsv}\")\n",
        "\n",
        "save_translations(sample_texts, translated_sample, out_txt=\"es.txt\", out_tsv=\"eng.tsv\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
