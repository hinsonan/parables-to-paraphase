{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f03104",
   "metadata": {},
   "source": [
    "# T5 VS Gemma-3-4B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7007372",
   "metadata": {},
   "source": [
    "## Setup & Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e97bc3",
   "metadata": {},
   "source": [
    "Environment configuration and GPU detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce553c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate sentencepiece torch --quiet\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8fb5c",
   "metadata": {},
   "source": [
    "Model initialization with memory tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64836c1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import time\n",
    "import pandas as pd\n",
    "t5_name = \"t5-small\"\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_name)\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(device)\n",
    "gemma_name = \"google/gemma-3-4b\"\n",
    "\n",
    "try:\n",
    "    gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_name)\n",
    "    gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "        gemma_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\")\n",
    "except Exception as e:\n",
    "    print(\"Could not load Gemma-3-4B:\", e)\n",
    "    gemma_tokenizer, gemma_model = None, None\n",
    "\n",
    "prompts = [\n",
    "    \"Short lesson (500-1000 words)\",\n",
    "    \"Medium lesson (2000-3000 words)\",\n",
    "    \"Long lesson (5000+ words)\"\n",
    "]\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b659c61",
   "metadata": {},
   "source": [
    "Baseline memory usage measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715d4f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    row = {\"prompt\": prompt}\n",
    "    t5_inputs = t5_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    start = time.time()\n",
    "    t5_outputs = t5_model.generate(**t5_inputs, max_new_tokens=64)\n",
    "    end = time.time()\n",
    "    row[\"t5_output\"] = t5_tokenizer.decode(t5_outputs[0], skip_special_tokens=True)\n",
    "    row[\"t5_runtime\"] = end - start\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "\n",
    "    if gemma_model:\n",
    "        gemma_inputs = gemma_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        start = time.time()\n",
    "        gemma_outputs = gemma_model.generate(**gemma_inputs, max_new_tokens=64)\n",
    "        end = time.time()\n",
    "        row[\"gemma_output\"] = gemma_tokenizer.decode(gemma_outputs[0], skip_special_tokens=True)\n",
    "        row[\"gemma_runtime\"] = end - start\n",
    "    else:\n",
    "        row[\"gemma_output\"] = \"<Gemma not loaded>\"\n",
    "        row[\"gemma_runtime\"] = None\n",
    "    \n",
    "    results.append(row)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577cc0b",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f81fb",
   "metadata": {},
   "source": [
    "Memory usage comparison charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11caeebd",
   "metadata": {},
   "source": [
    "Inference speed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bffb864",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(df.index - 0.15, df[\"t5_runtime\"], width=0.3, label=\"T5\")\n",
    "plt.bar(df.index + 0.15, df[\"gemma_runtime\"], width=0.3, label=\"Gemma\")\n",
    "plt.xticks(df.index, [f\"Prompt {i+1}\" for i in df.index])\n",
    "plt.ylabel(\"Runtime (s)\")\n",
    "plt.title(\"Inference Speed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9a1ef",
   "metadata": {},
   "source": [
    "Output length and generation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f91f85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(df[\"t5_tokens\"], df[\"t5_runtime\"], label=\"T5\", marker=\"o\")\n",
    "plt.scatter(df[\"gemma_tokens\"], df[\"gemma_runtime\"], label=\"Gemma\", marker=\"x\")\n",
    "plt.xlabel(\"Output tokens\")\n",
    "plt.ylabel(\"Runtime (s)\")\n",
    "plt.title(\"Output Length vs Generation Time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(df.index - 0.15, [m/1e6 for m in df[\"t5_mem_peak\"]], width=0.3, label=\"T5\")\n",
    "    plt.bar(df.index + 0.15, [m/1e6 for m in df[\"gemma_mem_peak\"]], width=0.3, label=\"Gemma\")\n",
    "    plt.xticks(df.index, [f\"Prompt {i+1}\" for i in df.index])\n",
    "    plt.ylabel(\"Peak Memory (MB)\")\n",
    "    plt.title(\"Memory Usage\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e12eb2",
   "metadata": {},
   "source": [
    "Batch processing performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58983d8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "batch_prompts = [\"Hello world!\" for _ in range(4)]\n",
    "batch_sizes = [1, 2, 4]\n",
    "batch_results = []\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    inputs = t5_tokenizer(batch_prompts[:bs], return_tensors=\"pt\", padding=True).to(device)\n",
    "    start = time.time()\n",
    "    _ = t5_model.generate(**inputs, max_new_tokens=32)\n",
    "    t5_time = time.time() - start\n",
    "\n",
    "    if gemma_model:\n",
    "        inputs = gemma_tokenizer(batch_prompts[:bs], return_tensors=\"pt\", padding=True).to(device)\n",
    "        start = time.time()\n",
    "        _ = gemma_model.generate(**inputs, max_new_tokens=32)\n",
    "        gemma_time = time.time() - start\n",
    "    else:\n",
    "        gemma_time = None\n",
    "\n",
    "    batch_results.append({\"batch_size\": bs, \"t5_time\": t5_time, \"gemma_time\": gemma_time})\n",
    "\n",
    "batch_df = pd.DataFrame(batch_results)\n",
    "display(batch_df)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(batch_df[\"batch_size\"], batch_df[\"t5_time\"], label=\"T5\", marker=\"o\")\n",
    "if gemma_model:\n",
    "    plt.plot(batch_df[\"batch_size\"], batch_df[\"gemma_time\"], label=\"Gemma\", marker=\"x\")\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Runtime (s)\")\n",
    "plt.title(\"Batch Processing Performance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe782b",
   "metadata": {},
   "source": [
    "## Qualitative Output Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336a07c",
   "metadata": {},
   "source": [
    "Side-by-side summary comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911b207",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "summary_cols = [\n",
    "    \"t5_runtime\", \"gemma_runtime\",\n",
    "    \"t5_tokens\", \"gemma_tokens\",\n",
    "    \"t5_mem_peak\", \"gemma_mem_peak\"\n",
    "]\n",
    "\n",
    "summary_df = df[[\"prompt\"] + summary_cols]\n",
    "summary_df\n",
    "\n",
    "comparison = pd.concat([\n",
    "    df[[\"prompt\", \"t5_runtime\", \"t5_tokens\", \"t5_mem_peak\"]].rename(\n",
    "        columns={\"t5_runtime\": \"runtime\", \"t5_tokens\": \"tokens\", \"t5_mem_peak\": \"peak_mem\"}\n",
    "    ).assign(model=\"T5\"),\n",
    "    \n",
    "    df[[\"prompt\", \"gemma_runtime\", \"gemma_tokens\", \"gemma_mem_peak\"]].rename(\n",
    "        columns={\"gemma_runtime\": \"runtime\", \"gemma_tokens\": \"tokens\", \"gemma_mem_peak\": \"peak_mem\"}\n",
    "    ).assign(model=\"Gemma\")\n",
    "])\n",
    "\n",
    "comparison = comparison.pivot(index=\"prompt\", columns=\"model\", values=[\"runtime\", \"tokens\", \"peak_mem\"])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664c1df",
   "metadata": {},
   "source": [
    "## Practical Recomendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881dd6b",
   "metadata": {},
   "source": [
    "Hardware requirement guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c34e0ad",
   "metadata": {},
   "source": [
    "- **T5 (small/base/large)**\n",
    "  - Memory footprint: 220M – 770M parameters.\n",
    "  - Runs comfortably on CPU, GPU optional for speedup.\n",
    "  - Peak GPU memory: ~1–2 GB for `t5-small`, ~4–6 GB for `t5-large`.\n",
    "  - Faster inference on modest hardware, but lower output fluency.\n",
    "\n",
    "- **Gemma-3-4B**\n",
    "  - 4B parameters → heavier model.\n",
    "  - GPU strongly recommended: needs ~8–12 GB VRAM for inference.\n",
    "  - CPU-only is possible but very slow.\n",
    "  - Better fluency, coherence, and longer-context handling, but higher hardware cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a33a2",
   "metadata": {},
   "source": [
    "Performance vs quality trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a91ff",
   "metadata": {},
   "source": [
    "- **T5**\n",
    "  - Pros: Fast, lightweight, lower memory usage, good for translation and summarization tasks.\n",
    "  - Cons: Output may be shorter, less nuanced, and sometimes rigid compared to larger models.\n",
    "\n",
    "- **Gemma-3-4B**\n",
    "  - Pros: Generates richer, more natural text; handles open-ended prompts better.\n",
    "  - Cons: Slower inference, higher VRAM usage, may require batching limits.\n",
    "\n",
    "Choose T5 for speed and low resources, and Gemma for quality and richer outputs if you have the hardware budget."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
