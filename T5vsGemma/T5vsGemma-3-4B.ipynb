{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f03104",
   "metadata": {},
   "source": [
    "# T5 VS Gemma-3-4B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7007372",
   "metadata": {},
   "source": [
    "## Setup & Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e97bc3",
   "metadata": {},
   "source": [
    "Environment configuration and GPU detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce553c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Device Used: cuda\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers accelerate pandas sentencepiece hf_xet torch torchvision --index-url https://download.pytorch.org/whl/cu126 --quiet\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device Used: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8fb5c",
   "metadata": {},
   "source": [
    "Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64836c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evand\\miniconda3\\envs\\p2p\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\evand\\.cache\\huggingface\\hub\\models--google--gemma-3-4b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 2 files: 100%|██████████| 2/2 [01:52<00:00, 56.17s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "t5_name = \"t5-small\"\n",
    "\n",
    "try:\n",
    "    t5_tokenizer = AutoTokenizer.from_pretrained(t5_name)\n",
    "    t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load {t5_name}:\", e)\n",
    "    t5_tokenizer, t5_model = None, None\n",
    "\n",
    "gemma_name = \"google/gemma-3-4b-it\"\n",
    "#it was chosen over pt because it is trained on more data and is supposed to be better suited for following instructions\n",
    "\n",
    "try:\n",
    "    gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_name)\n",
    "    gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "        gemma_name,\n",
    "        torch_dtype=torch.bfloat16\n",
    "        ).to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load {gemma_name}:\", e)\n",
    "    gemma_tokenizer, gemma_model = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472fe90",
   "metadata": {},
   "source": [
    "Data Loading and Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/parables.txt\", \"r\") as f:\n",
    "    prompts = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577cc0b",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_me(func: function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        return result, end - start\n",
    "    return wrapper\n",
    "\n",
    "def memory_me(func: function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start_mem = torch.cuda.memory_allocated()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_mem = torch.cuda.memory_allocated()\n",
    "        peak_mem = torch.cuda.max_memory_allocated()\n",
    "        return result, end_mem - start_mem, peak_mem\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_me\n",
    "@memory_me\n",
    "def summarize(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa72b6a1",
   "metadata": {},
   "source": [
    "Single Use Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dadca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_results = pd.DataFrame({\"prompt\": prompts, \"output\": [None]*len(prompts), \"inference_speed\": [None]*len(prompts), \"peak_memory\": [None]*len(prompts)})\n",
    "\n",
    "gemma_results = t5_results.copy()\n",
    "\n",
    "for prompt in prompts:\n",
    "    row_index = t5_results[t5_results[\"prompt\"] == prompt].index[0]\n",
    "\n",
    "    if t5_model :\n",
    "        t5_output, t5_runtime = summarize(t5_model, t5_tokenizer, prompt)\n",
    "        t5_results.at[row_index, \"output\"] = t5_output\n",
    "        t5_results.at[row_index, \"inference_speed\"] = t5_runtime\n",
    "    else:\n",
    "        t5_results.at[row_index, \"output\"] = \"<T5 not loaded>\"\n",
    "        t5_results.at[row_index, \"inference_speed\"] = None\n",
    "\n",
    "    if gemma_model:\n",
    "        gemma_output, gemma_runtime = summarize(gemma_model, gemma_tokenizer, \"Summarize this text: \" + prompt)\n",
    "        gemma_results.at[row_index, \"output\"] = gemma_output\n",
    "        gemma_results.at[row_index, \"inference_speed\"] = gemma_runtime\n",
    "    else:\n",
    "        gemma_results.at[row_index, \"output\"] = \"<Gemma not loaded>\"\n",
    "        gemma_results.at[row_index, \"inference_speed\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec79eb",
   "metadata": {},
   "source": [
    "Batch Processing Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f81fb",
   "metadata": {},
   "source": [
    "Memory usage comparison charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11caeebd",
   "metadata": {},
   "source": [
    "Inference speed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bffb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(df.index - 0.15, df[\"t5_runtime\"], width=0.3, label=\"T5\")\n",
    "plt.bar(df.index + 0.15, df[\"gemma_runtime\"], width=0.3, label=\"Gemma\")\n",
    "plt.xticks(df.index, [f\"Prompt {i+1}\" for i in df.index])\n",
    "plt.ylabel(\"Runtime (s)\")\n",
    "plt.title(\"Inference Speed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9a1ef",
   "metadata": {},
   "source": [
    "Output length and generation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f91f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(df[\"t5_tokens\"], df[\"t5_runtime\"], label=\"T5\", marker=\"o\")\n",
    "plt.scatter(df[\"gemma_tokens\"], df[\"gemma_runtime\"], label=\"Gemma\", marker=\"x\")\n",
    "plt.xlabel(\"Output tokens\")\n",
    "plt.ylabel(\"Runtime (s)\")\n",
    "plt.title(\"Output Length vs Generation Time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(df.index - 0.15, [m/1e6 for m in df[\"t5_mem_peak\"]], width=0.3, label=\"T5\")\n",
    "    plt.bar(df.index + 0.15, [m/1e6 for m in df[\"gemma_mem_peak\"]], width=0.3, label=\"Gemma\")\n",
    "    plt.xticks(df.index, [f\"Prompt {i+1}\" for i in df.index])\n",
    "    plt.ylabel(\"Peak Memory (MB)\")\n",
    "    plt.title(\"Memory Usage\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e12eb2",
   "metadata": {},
   "source": [
    "Batch processing performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58983d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prompts = [\"Hello world!\" for _ in range(4)]\n",
    "batch_sizes = [1, 2, 4]\n",
    "batch_results = []\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    inputs = t5_tokenizer(batch_prompts[:bs], return_tensors=\"pt\", padding=True).to(device)\n",
    "    start = time.time()\n",
    "    _ = t5_model.generate(**inputs, max_new_tokens=32)\n",
    "    t5_time = time.time() - start\n",
    "\n",
    "    if gemma_model:\n",
    "        inputs = gemma_tokenizer(batch_prompts[:bs], return_tensors=\"pt\", padding=True).to(device)\n",
    "        start = time.time()\n",
    "        _ = gemma_model.generate(**inputs, max_new_tokens=32)\n",
    "        gemma_time = time.time() - start\n",
    "    else:\n",
    "        gemma_time = None\n",
    "\n",
    "    batch_results.append({\"batch_size\": bs, \"t5_time\": t5_time, \"gemma_time\": gemma_time})\n",
    "\n",
    "batch_df = pd.DataFrame(batch_results)\n",
    "display(batch_df)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(batch_df[\"batch_size\"], batch_df[\"t5_time\"], label=\"T5\", marker=\"o\")\n",
    "if gemma_model:\n",
    "    plt.plot(batch_df[\"batch_size\"], batch_df[\"gemma_time\"], label=\"Gemma\", marker=\"x\")\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Runtime (s)\")\n",
    "plt.title(\"Batch Processing Performance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe782b",
   "metadata": {},
   "source": [
    "## Qualitative Output Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336a07c",
   "metadata": {},
   "source": [
    "Side-by-side summary comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cols = [\n",
    "    \"t5_runtime\", \"gemma_runtime\",\n",
    "    \"t5_tokens\", \"gemma_tokens\",\n",
    "    \"t5_mem_peak\", \"gemma_mem_peak\"\n",
    "]\n",
    "\n",
    "summary_df = df[[\"prompt\"] + summary_cols]\n",
    "summary_df\n",
    "\n",
    "comparison = pd.concat([\n",
    "    df[[\"prompt\", \"t5_runtime\", \"t5_tokens\", \"t5_mem_peak\"]].rename(\n",
    "        columns={\"t5_runtime\": \"runtime\", \"t5_tokens\": \"tokens\", \"t5_mem_peak\": \"peak_mem\"}\n",
    "    ).assign(model=\"T5\"),\n",
    "    \n",
    "    df[[\"prompt\", \"gemma_runtime\", \"gemma_tokens\", \"gemma_mem_peak\"]].rename(\n",
    "        columns={\"gemma_runtime\": \"runtime\", \"gemma_tokens\": \"tokens\", \"gemma_mem_peak\": \"peak_mem\"}\n",
    "    ).assign(model=\"Gemma\")\n",
    "])\n",
    "\n",
    "comparison = comparison.pivot(index=\"prompt\", columns=\"model\", values=[\"runtime\", \"tokens\", \"peak_mem\"])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664c1df",
   "metadata": {},
   "source": [
    "## Practical Recomendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881dd6b",
   "metadata": {},
   "source": [
    "Hardware requirement guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c34e0ad",
   "metadata": {},
   "source": [
    "- **T5 (small/base/large)**\n",
    "  - Memory footprint: 220M – 770M parameters.\n",
    "  - Runs comfortably on CPU, GPU optional for speedup.\n",
    "  - Peak GPU memory: ~1–2 GB for `t5-small`, ~4–6 GB for `t5-large`.\n",
    "  - Faster inference on modest hardware, but lower output fluency.\n",
    "\n",
    "- **Gemma-3-4B**\n",
    "  - 4B parameters → heavier model.\n",
    "  - GPU strongly recommended: needs ~8–12 GB VRAM for inference.\n",
    "  - CPU-only is possible but very slow.\n",
    "  - Better fluency, coherence, and longer-context handling, but higher hardware cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a33a2",
   "metadata": {},
   "source": [
    "Performance vs quality trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a91ff",
   "metadata": {},
   "source": [
    "- **T5**\n",
    "  - Pros: Fast, lightweight, lower memory usage, good for translation and summarization tasks.\n",
    "  - Cons: Output may be shorter, less nuanced, and sometimes rigid compared to larger models.\n",
    "\n",
    "- **Gemma-3-4B**\n",
    "  - Pros: Generates richer, more natural text; handles open-ended prompts better.\n",
    "  - Cons: Slower inference, higher VRAM usage, may require batching limits.\n",
    "\n",
    "Choose T5 for speed and low resources, and Gemma for quality and richer outputs if you have the hardware budget."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
